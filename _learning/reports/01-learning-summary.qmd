---
title: "Prompt Engineering 학습 정리"
author: "Jimin"
date: "2026-01-09"
format:
  html:
    code-fold: true
---

## 개요

이 문서는 [Prompt Engineering Guide](https://github.com/dair-ai/Prompt-Engineering-Guide) 레포를 기반으로 학습한 프롬프트 엔지니어링 핵심 개념을 정리합니다.

**학습 목표**: scrape-hub 프로젝트의 GPT-5 검증 파이프라인 구축

- NER 오매칭 필터링 (Gazetteer false positive 제거)
- Entity Linking 검증
- LLM-as-a-Judge 기반 품질 평가

---

## 1. 프롬프트 기초

### 프롬프트 구성 요소

| 요소 | 설명 | 예시 |
|------|------|------|
| **Instruction** | 수행할 작업 지시 | "다음 텍스트를 분류하세요" |
| **Context** | 배경 정보, 도메인 지식 | "의료 NER 검증 전문가입니다" |
| **Input Data** | 처리할 데이터 | `{text}`, `{entity}` |
| **Output Format** | 출력 형식 지정 | "유효 또는 무효로만 답하세요" |

### 모델 파라미터

```yaml
model:
  temperature: 0.1    # 낮을수록 일관성 ↑
  max_tokens: 50      # 출력 길이 제한
  top_p: 1            # nucleus sampling
```

- **Temperature**: 0.1 (분류/검증), 0.7+ (창작)
- **Max Tokens**: 작업에 맞게 설정 (비용 절감)

---

## 2. Few-Shot Prompting

### 핵심 개념

예시(exemplar)를 통해 모델에게 패턴을 학습시키는 기법

```
예시 1: 입력 → 출력
예시 2: 입력 → 출력
...
실제 질문: 입력 → ?
```

### 적용 사례: NER 검증

```yaml
## Biomarker 예시
텍스트: "HER2 양성 유방암 환자"
매칭: "HER2" → Biomarker
판정: 유효

텍스트: "cancer 관련 연구"
매칭: "er" → Biomarker
판정: 무효  # "cancer"의 부분 문자열
```

### 설계 팁

1. **다양성**: 각 클래스별 예시 포함
2. **경계 케이스**: 틀리기 쉬운 예시 포함
3. **균형**: 유효/무효 비율 맞추기

---

## 3. Chain-of-Thought (CoT)

### 핵심 개념

중간 추론 단계를 명시적으로 포함시켜 복잡한 문제 해결

```
일반: 질문 → 답
CoT:  질문 → 추론과정 → 답
```

### 방식 비교

| 방식 | 설명 | 사용법 |
|------|------|--------|
| **Few-shot CoT** | 예시에 추론 포함 | 예시 작성 시 추론 과정 포함 |
| **Zero-shot CoT** | 마법 문구 추가 | "단계별로 생각해보자" |
| **Auto-CoT** | 자동 추론 생성 | 클러스터링 + Zero-shot |

### 적용 사례

```yaml
# v3 프롬프트 (CoT)
텍스트: "cancer 관련 연구"
매칭: "er" → Biomarker
추론: "er"은 "cancer"의 일부분이며 독립 단어가 아님.
      소문자이고 ER 수용체 맥락 없음
판정: 무효
```

### 장단점

| 장점 | 단점 |
|------|------|
| 투명성 (왜 그런지 알 수 있음) | 토큰 사용량 증가 |
| 디버깅 용이 | 응답 시간 증가 |
| 복잡한 추론 가능 | max_tokens 늘려야 함 |

---

## 4. LLM-as-a-Judge

### 핵심 개념

LLM을 평가자로 사용하여 다른 LLM의 출력을 평가

### 평가 패턴

| 패턴 | 설명 |
|------|------|
| **Pointwise** | 단일 응답을 점수로 평가 (1-5점) |
| **Pairwise** | 두 응답 중 더 나은 것 선택 |
| **Reference-based** | 정답과 비교하여 평가 |
| **Multi-criteria** | 여러 기준으로 각각 평가 |

### 적용 사례: NER 검증 품질 평가

```json
{
  "accuracy_score": 5,      // 판정 정확성
  "reasoning_score": 4,     // 근거 품질
  "medical_score": 5,       // 의료 지식 정확성
  "total_score": 4.67,
  "verdict_correct": true,
  "feedback": "추론 과정이 명확하나 더 구체적인 근거 제시 가능"
}
```

### 주의할 편향

- **장황함 편향**: 긴 응답 선호 → 길이 정규화
- **위치 편향**: 첫 번째 옵션 선호 → 순서 랜덤화
- **자기 선호**: 자신의 출력 선호 → 다른 모델로 평가

---

## 5. 프롬프트 버전 관리

### YAML 기반 프롬프트 정의

```yaml
name: ner_validation
version: 2
created: 2026-01-09
author: jimin
description: "Gazetteer 오매칭 필터링"

model:
  name: gpt-4o-mini
  temperature: 0.1
  max_tokens: 50

messages:
  - role: system
    content: |
      당신은 의료 NER 검증 전문가입니다...
  - role: user
    content: |
      텍스트: "{text}"
      매칭: "{entity}" → {entity_type}
      판정:

variables:
  - name: text
  - name: entity
  - name: entity_type

changelog:
  - version: 1
    changes: "초기 버전"
  - version: 2
    changes: "모든 타입별 예시 추가"
```

### 장점

- Git으로 버전 관리
- 실험 재현성 보장
- 변경 이력 추적

---

## 요약

| 기법 | 용도 | 비용 |
|------|------|------|
| **Few-Shot** | 패턴 학습, 분류 | 낮음 |
| **CoT** | 복잡한 추론, 디버깅 | 중간 |
| **LLM-as-Judge** | 품질 평가, 모니터링 | 높음 |

### scrape-hub 적용 계획

1. **1차 검증**: v2 (Few-shot) - 대량 배치 처리
2. **디버깅 필요 시**: v3 (CoT) - 추론 과정 확인
3. **품질 모니터링**: v4 (Judge) - 샘플링 기반 QA
